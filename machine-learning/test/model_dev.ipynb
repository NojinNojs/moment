{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204488f4",
   "metadata": {},
   "source": [
    "# Transaction Classifier Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc7c0a",
   "metadata": {},
   "source": [
    "This notebook builds a bilingual transaction classification model that categorizes financial text inputs into 15 predefined income categories (e.g., Salary, Bonus, Investment) using a deep learning architecture combining BiLSTM, CNN, and an attention mechanism. We preprocess the data using a Keras tokenizer and label encoder, and leverage pre-trained FastText embeddings for both English and Indonesian to support bilingual inputs. The model is trained on padded sequences and evaluated on a test set for accuracy. Finally, we save the trained model along with the tokenizer and label encoder to enable seamless inference on new transaction data.\n",
    "\n",
    "`Notebook Author:` *Zev Hadid Santoso & Christian Julianto Sayono*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb857c8d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9675d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numpy pandas matplotlib tensorflow\n",
    "\n",
    "# Imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Embedding, LSTM, Bidirectional, Conv1D,\n",
    "                                     GlobalMaxPooling1D, Dense, Dropout, Concatenate)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe599f3",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc230a",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30dff0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('transactions.csv').dropna(subset=['text', 'label'])\n",
    "texts = df['text'].astype(str).tolist()\n",
    "labels = df['label'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f26ff",
   "metadata": {},
   "source": [
    "### Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac92f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().plot(kind='bar', title='Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc3081",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f409f3",
   "metadata": {},
   "source": [
    "### Download FastText Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da0f53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_fasttext(url, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    if not os.path.exists(output_path):\n",
    "        gz_path = output_path + '.gz'\n",
    "        print(f\"Downloading: {url}\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(gz_path, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        print(\"Extracting...\")\n",
    "        with gzip.open(gz_path, 'rb') as f_in:\n",
    "            with open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(gz_path)\n",
    "        print(f\"Saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"{output_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "en_path = \"../FastText_models/cc.en.300.vec\"\n",
    "id_path = \"../FastText_models/cc.id.300.vec\"\n",
    "\n",
    "# URLs\n",
    "en_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
    "id_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\"\n",
    "\n",
    "# Download models\n",
    "download_and_extract_fasttext(en_url, en_path)\n",
    "download_and_extract_fasttext(id_url, id_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943ad2e",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc94af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fc8ca",
   "metadata": {},
   "source": [
    "### Label Encoding & Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "764a99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "allowed_labels = ['Allowance', 'Asset Sale', 'Bonus', 'Dividend', 'Freelance',\n",
    "                  'Gift', 'Holiday Bonus', 'Inheritance', 'Investment',\n",
    "                  'Pension', 'Refund', 'Rental', 'Salary', 'Small Business']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(allowed_labels)\n",
    "y = label_encoder.transform(labels)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(allowed_labels))\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f10f9",
   "metadata": {},
   "source": [
    "### Load & Merge FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0203f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText embeddings\n",
    "def load_fasttext_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.asarray(parts[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embedding_en = load_fasttext_embeddings(en_path)\n",
    "embedding_id = load_fasttext_embeddings(id_path)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    vec = embedding_id.get(word)\n",
    "    if vec is None:\n",
    "        vec = embedding_en.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6d8a1",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b67efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom attention layer\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        e = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        alpha = K.softmax(e, axis=1)\n",
    "        context = inputs * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "    \n",
    "# Build model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_len,\n",
    "                            trainable=False)(input_layer)\n",
    "\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "cnn_out = Conv1D(64, 3, activation='relu')(lstm_out)\n",
    "cnn_out = GlobalMaxPooling1D()(cnn_out)\n",
    "\n",
    "attention_out = AttentionLayer()(lstm_out)\n",
    "\n",
    "combined = Concatenate()([cnn_out, attention_out])\n",
    "combined = Dropout(0.3)(combined)\n",
    "output = Dense(len(allowed_labels), activation='softmax')(combined)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27886e4",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ff3c8",
   "metadata": {},
   "source": [
    "## Save Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "folder_path = 'model_artifacts'\n",
    "tokenizer_path = os.path.join(folder_path, 'tokenizer.pkl')\n",
    "label_encoder_path = os.path.join(folder_path, 'label_encoder.pkl')\n",
    "model_path = os.path.join(folder_path, \"transaction_classifier_model.keras\")\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7f948",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583aec91",
   "metadata": {},
   "source": [
    "### Load Saved Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load for inference\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(model_path, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "\n",
    "with open(tokenizer_path, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open(label_encoder_path, 'rb') as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e3d3",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefbf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_transaction(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=model.input_shape[1], padding='post')\n",
    "    pred = model.predict(padded)\n",
    "    label_index = np.argmax(pred)\n",
    "    label = label_encoder.inverse_transform([label_index])[0]\n",
    "    return label, float(np.max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9baa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples\n",
    "sample_text_en = \"pension from the government\"\n",
    "label_en, conf_en = predict_transaction(sample_text_en)\n",
    "print(f\"Prediction: {label_en} ({conf_en:.2%} confidence)\")\n",
    "\n",
    "sample_text_id = \"menyimpan uang di bank\"\n",
    "label_id, conf_id = predict_transaction(sample_text_id)\n",
    "print(f\"Prediction: {label_id} ({conf_id:.2%} confidence)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
