{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmA1zqIn1enx"
      },
      "source": [
        "# Text Classification Using The LSTM Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpaq-5Cy1ieF"
      },
      "source": [
        "## Import necesarry Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUpw3C1Z79cy",
        "outputId": "2ba4acf0-2590-4304-9915-8a5a9e6cc5b0"
      },
      "outputs": [],
      "source": [
        "!pip install sastrawi pandas numpy tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DnokT83J1o3o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import pickle\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-A84iEN1sE0"
      },
      "source": [
        "## Deleting Model Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TCUcRy922n1P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed previous model_artifacts/finance_model.h5\n",
            "Removed previous model_artifacts/tokenizer.pkl\n",
            "Removed previous model_artifacts/label_mappings.pkl\n"
          ]
        }
      ],
      "source": [
        "artifacts = ['model_artifacts/finance_model.h5', 'model_artifacts/tokenizer.pkl', 'model_artifacts/label_mappings.pkl']\n",
        "\n",
        "for file in artifacts:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"Removed previous {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGSSrI0T2qoQ"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wVugyNPw2wRt"
      },
      "outputs": [],
      "source": [
        "factory = StemmerFactory()\n",
        "lemmatizer = factory.create_stemmer()\n",
        "\n",
        "FINANCIAL_TERMS = {\n",
        "    # Payment methods\n",
        "    \"gopay\": \"gopay\", \"ovo\": \"ovo\", \"dana\": \"dana\", \"shopeepay\": \"shopeepay\",\n",
        "    # Banks\n",
        "    \"bca\": \"bca\", \"bni\": \"bni\", \"bri\": \"bri\", \"mandiri\": \"mandiri\",\n",
        "    # Financial terms\n",
        "    \"kpr\": \"kpr\", \"atm\": \"atm\", \"rekening\": \"rekening\", \"deposito\": \"deposito\",\n",
        "    # Providers\n",
        "    \"pln\": \"pln\", \"pdam\": \"pdam\", \"telkomsel\": \"telkomsel\", \"indihome\": \"indihome\",\n",
        "    # Currencies\n",
        "    \"rp\": \"rp\", \"juta\": \"juta\", \"ribu\": \"ribu\",\n",
        "    # Modern terms (NEW)\n",
        "    \"bibit\": \"investasi\", \"pluang\": \"investasi\", \"stockbit\": \"investasi\",\n",
        "    \"fitness\": \"gym\", \"center\": \"gym\", \"membership\": \"member\",\n",
        "    \"top up\": \"isi ulang\", \"invest\": \"investasi\", \"saham\": \"investasi\"\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Enhanced preprocessing with English loanword handling\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace English financial terms (NEW)\n",
        "    replacements = {\n",
        "        'membership': 'member',\n",
        "        'fitness center': 'gym',\n",
        "        'bibit': 'aplikasi investasi',\n",
        "        'top up': 'isi ulang',\n",
        "        'invest': 'investasi'\n",
        "    }\n",
        "    for eng, ind in replacements.items():\n",
        "        text = text.replace(eng, ind)\n",
        "\n",
        "    # Remove special chars\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Token preservation and lemmatization\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token in FINANCIAL_TERMS:\n",
        "            tokens.append(FINANCIAL_TERMS[token])\n",
        "        else:\n",
        "            stemmed = lemmatizer.stem(token)\n",
        "            tokens.append(stemmed)\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaiLWJgN2ylP"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MI2OJ-1m24i5"
      },
      "outputs": [],
      "source": [
        "def load_and_augment_data(csv_path):\n",
        "    \"\"\"Load data with modern transaction augmentation\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Add modern transactions (NEW EXAMPLES)\n",
        "    modern_transactions = [\n",
        "        [\"Bayar member gym bulanan\", \"Health & Fitness\"],\n",
        "        [\"Pembayaran fitness center\", \"Health & Fitness\"],\n",
        "        [\"Investasi saham via Bibit\", \"Savings & Investments\"],\n",
        "        [\"Transfer BCA untuk cicilan rumah\", \"Debt & Loans\"],\n",
        "        [\"Top up investasi Bibit 1jt\", \"Savings & Investments\"],\n",
        "        [\"Belanja saham di Stockbit\", \"Savings & Investments\"],\n",
        "        [\"Bayar premi asuransi\", \"Utilities\"],\n",
        "        [\"Pembelian emas di Pluang\", \"Savings & Investments\"],\n",
        "        [\"Isi saldo Bibit\", \"Savings & Investments\"],\n",
        "        [\"Langganan gym premium\", \"Health & Fitness\"]\n",
        "    ]\n",
        "\n",
        "    modern_df = pd.DataFrame(modern_transactions, columns=['text', 'label'])\n",
        "    df = pd.concat([df, modern_df])\n",
        "\n",
        "    # Preprocess all text\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDLoMSiQ26pU"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zqegMJav2-yE"
      },
      "outputs": [],
      "source": [
        "def create_lstm_model(vocab_size, num_classes, max_len):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True, input_length=max_len),\n",
        "        Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)),\n",
        "        Bidirectional(LSTM(64, dropout=0.2)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2bxa-Gx3AlQ"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8VmekCGN3GUm"
      },
      "outputs": [],
      "source": [
        "def train_model(df, model_save_path='model_artifacts/finance_model.h5'):\n",
        "    # Prepare labels\n",
        "    label2id = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    y = pd.get_dummies(df['label'].map(label2id)).values\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\", filters='')\n",
        "    tokenizer.fit_on_texts(df['processed_text'])\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Sequence preparation\n",
        "    sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
        "    max_len = max(len(seq) for seq in sequences)\n",
        "    X = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(df['label'].map(label2id)),\n",
        "        y=df['label'].map(label2id)\n",
        "    )\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Model training\n",
        "    model = create_lstm_model(vocab_size, len(label2id), max_len)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        ModelCheckpoint(model_save_path, save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Save artifacts\n",
        "    with open('model_artifacts/tokenizer.pkl', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('model_artifacts/label_mappings.pkl', 'wb') as handle:\n",
        "        pickle.dump((label2id, id2label), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return model, tokenizer, label2id, id2label, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9069rWp63Qer"
      },
      "source": [
        "## Model Evaluation and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cuiLgNha3Upq"
      },
      "outputs": [],
      "source": [
        "class FinancialClassifier:\n",
        "    def __init__(self, model_path='model_artifacts/finance_model.h5'):\n",
        "        self.model = load_model(model_path)\n",
        "        with open('model_artifacts/tokenizer.pkl', 'rb') as handle:\n",
        "            self.tokenizer = pickle.load(handle)\n",
        "        with open('model_artifacts/label_mappings.pkl', 'rb') as handle:\n",
        "            self.label2id, self.id2label = pickle.load(handle)\n",
        "        self.max_len = self.model.input_shape[1]\n",
        "\n",
        "    def predict(self, text, confidence_threshold=0.7):\n",
        "        # Enhanced preprocessing\n",
        "        processed = preprocess_text(text)\n",
        "        seq = self.tokenizer.texts_to_sequences([processed])\n",
        "        padded = pad_sequences(seq, maxlen=self.max_len, padding='post', truncating='post')\n",
        "\n",
        "        # Predict\n",
        "        proba = self.model.predict(padded, verbose=0)[0]\n",
        "        pred_id = np.argmax(proba)\n",
        "        confidence = proba[pred_id]\n",
        "\n",
        "        # Prepare results\n",
        "        result = {\n",
        "            'original_text': text,\n",
        "            'processed_text': processed,\n",
        "            'prediction': self.id2label[pred_id],\n",
        "            'confidence': float(confidence),\n",
        "            'all_predictions': {\n",
        "                self.id2label[i]: float(p)\n",
        "                for i, p in enumerate(proba)\n",
        "                if p > 0.05\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Apply confidence threshold\n",
        "        if confidence < confidence_threshold:\n",
        "            result['prediction'] = 'LOW_CONFIDENCE'\n",
        "            result['suggestion'] = 'Needs manual review'\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lphb0i6B83N_",
        "outputId": "5e450379-80f2-4079-ce5a-1e13603cd628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 148 samples with 12 categories\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zevha\\Moment\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 8s/step - accuracy: 0.0625 - loss: 2.4764 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 408ms/step - accuracy: 0.0849 - loss: 2.4841 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2000 - val_loss: 2.4825 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1875 - loss: 2.5348 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2249 - loss: 2.4999 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2333 - val_loss: 2.4797 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1875 - loss: 2.5495 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2395 - loss: 2.4990 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2667 - val_loss: 2.4749 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0938 - loss: 2.4546 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2564 - loss: 2.4617 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.3333 - val_loss: 2.4676 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3750 - loss: 2.3298 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4159 - loss: 2.4110 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2667 - val_loss: 2.4556 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4062 - loss: 2.3879 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4299 - loss: 2.4168 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2333 - val_loss: 2.4383 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4062 - loss: 2.4847 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4390 - loss: 2.4232 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.3000 - val_loss: 2.4069 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5312 - loss: 2.3378 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4864 - loss: 2.3452 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2667 - val_loss: 2.3487 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4062 - loss: 2.2061 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4594 - loss: 2.2057 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2667 - val_loss: 2.2462 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5312 - loss: 2.0265 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5195 - loss: 2.0227 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2667 - val_loss: 2.1361 - val_precision: 0.5000 - val_recall: 0.0333\n",
            "Epoch 11/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6250 - loss: 1.7452 - precision: 1.0000 - recall: 0.0625"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5081 - loss: 1.7676 - precision: 0.9667 - recall: 0.0935 - val_accuracy: 0.2667 - val_loss: 2.0419 - val_precision: 0.2500 - val_recall: 0.0333\n",
            "Epoch 12/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5938 - loss: 1.5751 - precision: 1.0000 - recall: 0.2812"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5242 - loss: 1.5876 - precision: 0.9247 - recall: 0.2199 - val_accuracy: 0.4000 - val_loss: 1.9095 - val_precision: 0.5714 - val_recall: 0.1333\n",
            "Epoch 13/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6562 - loss: 1.2345 - precision: 0.9231 - recall: 0.3750"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5901 - loss: 1.3195 - precision: 0.8834 - recall: 0.3288 - val_accuracy: 0.4667 - val_loss: 1.8005 - val_precision: 0.7778 - val_recall: 0.2333\n",
            "Epoch 14/50\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5312 - loss: 1.2500 - precision: 0.8667 - recall: 0.4062"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5756 - loss: 1.1240 - precision: 0.9126 - recall: 0.4359 - val_accuracy: 0.5333 - val_loss: 1.6780 - val_precision: 0.7273 - val_recall: 0.2667\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7222 - loss: 0.9260 - precision: 0.9224 - recall: 0.4859 - val_accuracy: 0.5000 - val_loss: 1.7733 - val_precision: 0.6429 - val_recall: 0.3000\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7580 - loss: 0.8038 - precision: 0.9478 - recall: 0.5896 - val_accuracy: 0.5333 - val_loss: 1.7183 - val_precision: 0.6875 - val_recall: 0.3667\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8697 - loss: 0.5600 - precision: 0.9395 - recall: 0.6800 - val_accuracy: 0.6333 - val_loss: 1.7360 - val_precision: 0.6316 - val_recall: 0.4000\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8726 - loss: 0.4406 - precision: 0.9282 - recall: 0.7543 - val_accuracy: 0.6000 - val_loss: 1.7174 - val_precision: 0.6000 - val_recall: 0.5000\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9330 - loss: 0.3648 - precision: 0.9346 - recall: 0.8301 - val_accuracy: 0.6333 - val_loss: 1.7761 - val_precision: 0.6154 - val_recall: 0.5333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enhanced Test Predictions:\n",
            "\n",
            "Original: Gaji bulan Desember dari kantor\n",
            "Processed: gaji bulan desember dari kantor\n",
            "Prediction: Income (Confidence: 99.70%)\n",
            "Details:\n",
            "- Income: 99.70%\n",
            "\n",
            "Original: Bayar tagihan listrik PLN\n",
            "Processed: bayar tagih listrik pln\n",
            "Prediction: Utilities (Confidence: 71.33%)\n",
            "Details:\n",
            "- Debt & Loans: 14.22%\n",
            "- Utilities: 71.33%\n",
            "\n",
            "Original: Beli makan siang di warteg\n",
            "Processed: beli makan siang di warteg\n",
            "Prediction: Food & Dining (Confidence: 95.67%)\n",
            "Details:\n",
            "- Food & Dining: 95.67%\n",
            "\n",
            "Original: Transfer ke BCA untuk angsuran KPR\n",
            "Processed: transfer ke bca untuk angsur kpr\n",
            "Prediction: Debt & Loans (Confidence: 75.65%)\n",
            "Details:\n",
            "- Debt & Loans: 75.65%\n",
            "- Transportation: 17.62%\n",
            "\n",
            "Original: Isi pulsa Telkomsel 100rb\n",
            "Processed: isi pulsa telkomsel 100rb\n",
            "Prediction: LOW_CONFIDENCE (Confidence: 48.16%)\n",
            "⚠️ Needs manual review\n",
            "Details:\n",
            "- Debt & Loans: 5.20%\n",
            "- Gifts & Donations: 8.84%\n",
            "- Income: 8.26%\n",
            "- Miscellaneous: 10.60%\n",
            "- Savings & Investments: 7.11%\n",
            "- Utilities: 48.16%\n",
            "\n",
            "Original: Investasi saham di Bibit\n",
            "Processed: investasiasi investasi di aplikasi investasiasi\n",
            "Prediction: Savings & Investments (Confidence: 99.48%)\n",
            "Details:\n",
            "- Savings & Investments: 99.48%\n",
            "\n",
            "Original: Bayar membership fitness center\n",
            "Processed: bayar member gym\n",
            "Prediction: LOW_CONFIDENCE (Confidence: 19.74%)\n",
            "⚠️ Needs manual review\n",
            "Details:\n",
            "- Debt & Loans: 15.90%\n",
            "- Education: 5.59%\n",
            "- Entertainment: 10.63%\n",
            "- Health & Fitness: 8.73%\n",
            "- Miscellaneous: 6.14%\n",
            "- Transportation: 18.65%\n",
            "- Utilities: 19.74%\n",
            "\n",
            "Original: Donasi untuk korban bencana\n",
            "Processed: donasi untuk korban bencana\n",
            "Prediction: Gifts & Donations (Confidence: 96.97%)\n",
            "Details:\n",
            "- Gifts & Donations: 96.97%\n",
            "\n",
            "Original: Top up Bibit 500rb\n",
            "Processed: isi ulang aplikasi investasiasi 500rb\n",
            "Prediction: LOW_CONFIDENCE (Confidence: 61.10%)\n",
            "⚠️ Needs manual review\n",
            "Details:\n",
            "- Income: 61.10%\n",
            "- Savings & Investments: 28.69%\n",
            "- Utilities: 8.62%\n",
            "\n",
            "Original: Langganan gym premium\n",
            "Processed: langgan gym premium\n",
            "Prediction: LOW_CONFIDENCE (Confidence: 18.94%)\n",
            "⚠️ Needs manual review\n",
            "Details:\n",
            "- Debt & Loans: 7.71%\n",
            "- Education: 7.46%\n",
            "- Entertainment: 18.94%\n",
            "- Food & Dining: 7.86%\n",
            "- Health & Fitness: 10.73%\n",
            "- Savings & Investments: 7.58%\n",
            "- Shopping: 7.42%\n",
            "- Transportation: 15.27%\n",
            "- Utilities: 8.53%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # 1. Load and augment data\n",
        "        df = load_and_augment_data('datasets/transactions.csv')\n",
        "        print(f\"Loaded {len(df)} samples with {len(df['label'].unique())} categories\")\n",
        "\n",
        "        # 2. Train the model\n",
        "        print(\"\\nTraining model...\")\n",
        "        model, tokenizer, label2id, id2label, max_len = train_model(df)\n",
        "\n",
        "        # 3. Initialize classifier\n",
        "        classifier = FinancialClassifier()\n",
        "\n",
        "        # 4. Test predictions\n",
        "        test_cases = [\n",
        "            \"Gaji bulan Desember dari kantor\",\n",
        "            \"Bayar tagihan listrik PLN\",\n",
        "            \"Beli makan siang di warteg\",\n",
        "            \"Transfer ke BCA untuk angsuran KPR\",\n",
        "            \"Isi pulsa Telkomsel 100rb\",\n",
        "            \"Investasi saham di Bibit\",\n",
        "            \"Bayar membership fitness center\",\n",
        "            \"Donasi untuk korban bencana\",\n",
        "            \"Top up Bibit 500rb\",\n",
        "            \"Langganan gym premium\"\n",
        "        ]\n",
        "\n",
        "        print(\"\\nEnhanced Test Predictions:\")\n",
        "        for text in test_cases:\n",
        "            result = classifier.predict(text)\n",
        "            print(f\"\\nOriginal: {result['original_text']}\")\n",
        "            print(f\"Processed: {result['processed_text']}\")\n",
        "            print(f\"Prediction: {result['prediction']} (Confidence: {result['confidence']:.2%})\")\n",
        "            if 'suggestion' in result:\n",
        "                print(f\"⚠️ {result['suggestion']}\")\n",
        "            print(\"Details:\")\n",
        "            for cat, prob in result['all_predictions'].items():\n",
        "                if prob > 0.05:\n",
        "                    print(f\"- {cat}: {prob:.2%}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: File 'transactions.csv' not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
